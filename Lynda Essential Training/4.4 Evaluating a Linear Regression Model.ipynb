{"cells":[{"cell_type":"code","source":["%sh curl -O 'https://raw.githubusercontent.com/bsullins/bensullins.com-freebies/master/CogsleyServices-SalesData-US.csv'\n# saves file to file:/databricks/driver/CogsleyServices-SalesData-US.csv"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["path = 'file:/databricks/driver/CogsleyServices-SalesData-US.csv'\n# path = \"/databricks-datasets/samples/population-vs-price/data_geo.csv\"\n\n# Use the Spark CSV datasource with options specifying:\n# - First line of file is a header\n# - Automatically infer the schema of the data\ndata = sqlContext.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(path)\n \ndata.cache() # Cache data for faster reuse\ndata = data.dropna() # drop rows with missing values\n \n# Register table so it is accessible via SQL Context\n# For Apache Spark = 2.0\n# data.createOrReplaceTempView(\"data_geo\")\n\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Get monthly sales totals\nsummary = data.select(\"OrderMonthYear\", \"SaleAmount\").groupBy(\"OrderMonthYear\").sum().orderBy(\"OrderMonthYear\").toDF(\"OrderMonthYear\",\"SaleAmount\")\n\n# Convert OrderMonthYear to integer type\nresults = summary.map(lambda r: (int(r.OrderMonthYear.replace('-','')), r.SaleAmount)).toDF([\"OrderMonthYear\",\"SaleAmount\"])\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# convenience for specifying schema\nfrom pyspark.mllib.regression import LabeledPoint\n \ndata = results.select(\"OrderMonthYear\", \"SaleAmount\")\\\n  .map(lambda r: LabeledPoint(r[1], [r[0]]))\\\n  .toDF()\n  \ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Import LinearRegression class\nfrom pyspark.ml.regression import LinearRegression\n \n# Define LinearRegression algorithm\nlr = LinearRegression()\n \n# Fit 2 models, using different regularization parameters\nmodelA = lr.fit(data, {lr.regParam:0.0})\nmodelB = lr.fit(data, {lr.regParam:100.0})\n\n# Make predictions\npredictionsA = modelA.transform(data)\npredictionsB = modelB.transform(data)\n\ndisplay(predictionsA)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nevaluator = RegressionEvaluator(metricName=\"rmse\")\n\nRMSE = evaluator.evaluate(predictionsA)\nprint(\"ModelA: Root Mean Squared Error = \" + str(RMSE)) \n\nRMSE = evaluator.evaluate(predictionsB)\nprint(\"ModelB: Root Mean Squared Error = \" + str(RMSE))\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"4.4 Evaluating a Linear Regression Model","notebookId":2279204284361393},"nbformat":4,"nbformat_minor":0}
