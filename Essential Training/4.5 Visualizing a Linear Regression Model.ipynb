{"cells":[{"cell_type":"code","source":["%sh curl -O 'https://raw.githubusercontent.com/bsullins/bensullins.com-freebies/master/CogsleyServices-SalesData-US.csv'\n# saves file to file:/databricks/driver/CogsleyServices-SalesData-US.csv"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["path = 'file:/databricks/driver/CogsleyServices-SalesData-US.csv'\n# path = \"/databricks-datasets/samples/population-vs-price/data_geo.csv\"\n\n# Use the Spark CSV datasource with options specifying:\n# - First line of file is a header\n# - Automatically infer the schema of the data\ndata = sqlContext.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(path)\n \ndata.cache() # Cache data for faster reuse\ndata = data.dropna() # drop rows with missing values\n \n# Register table so it is accessible via SQL Context\n# For Apache Spark = 2.0\n# data.createOrReplaceTempView(\"data_geo\")\n\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Get monthly sales totals\nsummary = data.select(\"OrderMonthYear\", \"SaleAmount\").groupBy(\"OrderMonthYear\").sum().orderBy(\"OrderMonthYear\").toDF(\"OrderMonthYear\",\"SaleAmount\")\n\n# Convert OrderMonthYear to integer type\nresults = summary.map(lambda r: (int(r.OrderMonthYear.replace('-','')), r.SaleAmount)).toDF([\"OrderMonthYear\",\"SaleAmount\"])\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# convenience for specifying schema\nfrom pyspark.mllib.regression import LabeledPoint\n \ndata = results.select(\"OrderMonthYear\", \"SaleAmount\")\\\n  .map(lambda r: LabeledPoint(r[1], [r[0]]))\\\n  .toDF()\n  \ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Import LinearRegression class\nfrom pyspark.ml.regression import LinearRegression\n \n# Define LinearRegression algorithm\nlr = LinearRegression()\n \n# Fit 2 models, using different regularization parameters\nmodelA = lr.fit(data, {lr.regParam:0.0})\nmodelB = lr.fit(data, {lr.regParam:100.0})\n\n# Make predictions\npredictionsA = modelA.transform(data)\npredictionsB = modelB.transform(data)\n\ndisplay(predictionsA)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nevaluator = RegressionEvaluator(metricName=\"rmse\")\nRMSE = evaluator.evaluate(predictionsA)\nprint(\"ModelA: Root Mean Squared Error = \" + str(RMSE)) \n\nRMSE = evaluator.evaluate(predictionsB)\nprint(\"ModelB: Root Mean Squared Error = \" + str(RMSE))\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["\n# define column names\ncols = [\"OrderMonthYear\", \"SaleAmount\", \"Prediction\"]\n\n# use parallelize to create an RDD\n# use map() with lambda to parse features\ntableA = sc.parallelize(\\\n            predictionsA.map(lambda r: (float(r.features[0]), r.label, r.prediction)).collect()\\\n         ).toDF(cols) \n\n# repeate for modelB\ntableB = sc.parallelize(\\\n            predictionsB.map(lambda r: (float(r.features[0]), r.label, r.prediction)).collect()\\\n         ).toDF(cols) \n\n# check results\n# display(tableA)\n\n# save results as tables\ntableA.write.saveAsTable('predictionsA', mode='overwrite')\nprint \"Created predictionsA table\"\n\ntableB.write.saveAsTable('predictionsB', mode='overwrite')\nprint \"Created predictionsB table\"\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql \nselect \n    a.OrderMonthYear,\n    a.SaleAmount,\n    a.prediction as ModelA,\n    b.prediction as ModelB\nfrom predictionsA a\njoin predictionsB b on a.OrderMonthYear = b.OrderMonthYear\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"4.5 Visualizing a Linear Regression Model","notebookId":2279204284361455},"nbformat":4,"nbformat_minor":0}
